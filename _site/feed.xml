<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.5">Jekyll</generator><link href="http://localhost:4444/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4444/" rel="alternate" type="text/html" /><updated>2019-10-25T14:19:52-07:00</updated><id>http://localhost:4444/</id><title type="html">A Rambling On</title><subtitle>My exploration of neuroscience, algorithms, and statistics.</subtitle><author><name>Kristian M. Eschenburg</name></author><entry><title type="html">Dose-Response Curves and Biomarker Diagnostic Power</title><link href="http://localhost:4444/2019/08/dose-response" rel="alternate" type="text/html" title="Dose-Response Curves and Biomarker Diagnostic Power" /><published>2019-08-27T04:12:32-07:00</published><updated>2019-08-27T04:12:32-07:00</updated><id>http://localhost:4444/2019/08/dose-response</id><content type="html" xml:base="http://localhost:4444/2019/08/dose-response">The other day, one of my friends and fellow PhD colleagues (I'll refer to him as **Dr. A**) asked me if I knew anything about assessing biomarker diagnostic power.  He went on to describe his clinical problem.  I'll try to describe his research problem here (but will likely mess up some of the relevant details, as his research pertains to generating induced pluripotent cardiac stem cells, which I have little to no experience with):

- **Dr. A**:  
    - &quot;So chemotherapy is meant to combat cancer.  But some clinicians, and their patients, have found that some forms of chemotherapy and anti-cancer drugs later result in problems with the heart and vasculature -- these problems are collectively referred to as 'cardiotoxicity'.  
    
    - We're interested in developing biomarkers that will help us identify which patients might be susceptible to cardiotoxicity, and in assessing the predictive power of these biomarkers.  Can you help me?&quot;

- **Me**:
    &quot;Sure!&quot;

What follows will be my exploration into what **Dr. A** called [dose-response curves](https://en.wikipedia.org/wiki/Dose%E2%80%93response_relationship), and my approach on how to use these curves to assess biomarker diagnostic power.  I'll do a walk through of some Python code that I've written up, where I'll examine dose-response curves and their diagnostic power using the **receiver operating characteristic** ([ROC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)) and the closely related **area under the curve** ([AUC](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve)) metrics.

If you want to see the actual dose-response curve analysis, skip to the **Analyzing Synthetic Dose-Response Curves** section down below.

### Brief Introduction to the ROC and AUC
I first wanted to describe to **Dr. A** how to use the ROC for biomarkers, so I made a brief Python tutorial for him to look at.  Below begins a more in-depth description of what I sent him.

We start by importing some necessary libraries:
```python
%matplotlib inline
import matplotlib.pyplot as plt
from matplotlib.lines import Line2D

import numpy as np
from sklearn.metrics import auc
```

Next, we define a function to compute the ROC curve.  While [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) has a function to compute the ROC, computing it yourself makes it easier to understand what the ROC curve actually represents.  

The ROC curve is essentially a curve that plots the true positive and false positive classification rates against one another, for various user-defined thresholds of the data.  We pick a biomarker level threshold -- let's say $$T$$ -- assign each sample a value of 0 or 1 depending on whether its measured biomarker level is greater than or less than $$T$$, and then compare these assignments to the *true* Case/Control classifications to compute the true and false positive rates.  We plots these rates for many $$T$$ to generate the ROC curve.

The ROC let's you look at how sensitive your model is to various parameterizations -- are you able to accurately identify Cases from Controls?  How acceptable are misclassified results?  In this situation, we don't want to classify someone as healthy when in fact they might develop cardiotoxicity, so we want to allow some flexibility in terms of the number of false positives generated by our model -- it's a classic case of &quot;better safe than sorry&quot;, since the &quot;sorry&quot; outcome might be an accidental patient death.

```python
def roc(case, control, npoints, gte=True):
    
    &quot;&quot;&quot;
    Compute ROC curve for given set of Case/Control samples.
    
    Parameters:
    - - - - -
    case: float, array
        Samples from case patients
    control: float, array
        Samples from control patients
    npoints: int
        Number of TP/FP pairs to generate
    gte: boolean
        Whether control mean expected to be greater
        than case mean.

    Returns:
    - - - -
    specificity: float, array
        false positive rates
    sensitivity: float, array
        true positive rates
    &quot;&quot;&quot;

    # make sure generating more
    # than 1 TP/FP pair
    # so we can plot an actual ROC curve
    assert npoints &gt; 1
    

    # made sure case and control samples
    # are numpy arrays
    case = np.asarray(case)
    control = np.asarray(control)
    
    # check for NaN values
    # keep only indices without NaN
    case_nans = np.isnan(case)
    cont_nans = np.isnan(control)
    nans = (case_nans + cont_nans)
    
    specificity = []
    sensitivity = []
    
    # we'll define the min and max thresholds 
    # based on the min and max of our data
    conc = np.concatenate([case[~nans],control[~nans]])

    # function comparison map
    # use ```gte``` parameter
    # if we expect controls to be less than
    # cases, gte = False
    # otherwise gte = True
    comp_map = {'False': np.less,
                'True': np.greater}
    
    # generate npoints equally spaced threshold values
    # compute the false positive / true positive rates 
    # at each threshold
    for thresh in np.linspace(conc.min(), conc.max(), npoints):

        fp = (comp_map[gte](case[~nans], thresh)).mean()
        tn = 1-fp
        
        tp = (comp_map[gte](control[~nans], thresh)).mean()
        fn = 1-tp
        
        specificity.append(tn)
        sensitivity.append(tp)
    
    return [np.asarray(specificity), np.asarray(sensitivity)]
```

Next up, I generate 5 different datasets.  Each dataset corresponds to fake samples from a &quot;Control&quot; distribution, and a &quot;Case&quot; distribution -- each is distributed according to a univariate Normal distribution.  The Control distribution remains the same in each scenario: $$N(\mu = 10, \sigma = 1)$$, but I change the $$\mu_{Case}$$ parameter of the Case distribution in each instance, such that $$\mu_{Case} \in [5,6,7,8,9,10]$$.

We plot the example datasets as follows, and then compute the ROC curves for each dataset.
```python
# define unique means
m1 = np.arange(5,11)
samples = {}.fromkeys(m1)

# define control distribution (this stays the same across 
n2 = np.random.normal(loc=10, scale=1, size=2000)

fig, (ax) = plt.subplots(2, 3, figsize=(15, 6))

for i, ax in enumerate(fig.axes):

    n1 = np.random.normal(loc=m1[i], scale=1, size=2000)
    samples[m1[i]] = n1

    ax.hist(n1, 25, alpha=0.5, label='Case', density=True,)
    ax.hist(n2, 25, alpha=0.5, label='Control', density=True,)
    ax.set_title('Mean: {:}, Sigma: 1'.format(m1[i]), fontsize=15)
    ax.set_xlabel('Biomarker Measures Variable', fontsize=15)
    ax.set_ylabel('Density', fontsize=15)
    ax.legend()

plt.tight_layout()
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/Example.Histograms.jpg' class=&quot;center-image&quot; width=&quot;110%&quot;/&gt;
    &lt;figcaption&gt;Example Case/Control datasets.&lt;/figcaption&gt;
&lt;/figure&gt;

```python
fig, (ax1) = plt.subplots(1, 1)
for mean, case_data in samples.items():

    [spec, sens] = roc(case_data, n2, 100, gte=True)
    A = auc(spec, sens)

    L = 'Mean: %i, AUC: %.3f' % (mean, A)
    plt.plot(1-np.asarray(spec), np.asarray(sens), label=L)
    plt.legend(bbox_to_anchor=(1.04,1), fontsize=15)
    plt.xlabel('1-Specificity', fontsize=15)
    plt.ylabel('Sensitivity', fontsize=15)

    plt.title('ROC for Varying Case Distributions', fontsize=15)
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/Example.ROC.Curves.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;ROC curves for example datasets.&lt;/figcaption&gt;
&lt;/figure&gt;

We see that, as the distributions become less separated, the ability to distinguish points from either distribution is diminished.  This is shown by 1) a flattening of the ROC curve towards the diagonal, along with 2) the integration of the ROC curve, which generates the AUC metric.  When the distributions are far apart (as when the $$\mu_{Case} = 5$$), it is quite easy for a simple model to distinguish points sampled from either distribution, meaning this hypothetical model has good diagnostic power.

### Analyzing Synthetic Dose-Response Curves

We now analyze some synthetic data, generated to look like dose-response curves.  As a refresher, dose-response curves measure the behavior of some tissue cells in response to increasing levels of (generally) drugs.  The $$x$$-axis is the drug dose, measured in some concentration or volume, and the $$y$$-axis is generally some measure of cell death or survival, generally ranging from 0% to 100%.  The curves, however, look sigmoidal.  So let's first generate a function to create sigmoid curves.

```python
def sigmoid(beta, intercept, x):
    
    &quot;&quot;&quot;
    Fake sigmoid function, takes in coefficient, shift, and dose values.
    
    Parameters:
    - - - - -
    beta: float
        slope
    intercept: float
        negative exponential intercept
    x: float, array
        data samples
        
    Returns:
    - - - -
    dose_response: float, array
        single-subject dose-response vector
        Between 0 and 1.
    &quot;&quot;&quot;
    
    dose_resonse =  1 / (1 + np.exp(-beta*x + intercept))
    
    return dose_resonse
```

Next, let's actually generate some synethetic data for a dataset of fake subjects.  I want to incorporate some variability into the Cases and Controls, so I'll sample the subject parameters from distributions.  In this case, for each subject, I'll sample the logistic curve slope coefficient from $$Beta$$ distributions, and the intercept from $$Normal$$ distributions.  We'll sample 1000 Cases, and 1000 Controls.

For the slopes, we have

$$
\begin{align}
\beta_{Control} &amp;\sim Beta(a=5, b=3) \\
\beta_{Case} &amp;\sim Beta(a=10, b=2)
\end{align}
$$

and for the intercepts, we have


$$
\begin{align}
I_{Control} &amp;\sim Normal(\mu=0, \sigma=1) \\
I_{Case} &amp;\sim Normal(\mu=4, \sigma=1)
\end{align}
$$

As such, the dose-response curve for individual, $k$, is generated as follows:

$$
dr_{k} = \frac{1}{1 + e^{-(\beta_{k}X + I_{k})}}
$$

where $$\beta_{k}$$ and $$I_{k}$$ are the slope and intercept values for the given subject.  These distributional parameterizations are arbitrary -- I just wanted to be able to incorporate variability across subjects and groups.

Let's generate some random Case/Control dose-response data and plot the coefficient histograms:

```python
# n cases and controls
S = 1000

# dictionary of slopes and intercept values for each subject
controls = {k: {'beta': None,
                'intercept': None} for k in np.arange(S)}
cases = {k: {'beta': None,
             'intercept': None} for k in np.arange(S)}

# get lists of betas and intercepts
beta_control = []
beta_case = []

intercept_control = []
intercept_case = []

for i in np.arange(S):

    controls[i]['beta'] = np.random.beta(a=5, b=3)
    controls[i]['intercept'] = np.random.normal(loc=0, scale=1)

    intercept_control.append(controls[i]['intercept'])
    beta_control.append(controls[i]['beta'])

    cases[i]['beta'] = np.random.beta(a=10, b=2)
    cases[i]['intercept'] = np.random.normal(loc=4, scale=1)

    intercept_case.append(cases[i]['intercept'])
    beta_case.append(cases[i]['beta'])
```

Intercept histograms look like two different $$Normal$$ distributions:

```python
fig = plt.figure(figsize=(12, 8))
plt.hist(intercept_control, 50, color='r', alpha=0.5, density=True, label='Control');
plt.hist(intercept_case, 50, color='b', alpha=0.5, density=True, label='Case');
plt.ylabel('Density', fontsize=15);
plt.xlabel('Intercepts', fontsize=15);
plt.title('Intercepts Coefficients By Group', fontsize=15);
plt.legend(fontsize=15);
plt.tight_layout()
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/DR.Intercepts.jpg' class=&quot;center-image&quot; width=&quot;80%&quot;/&gt;
    &lt;figcaption&gt;Intercept distributions for synthetic dose-response curves.&lt;/figcaption&gt;
&lt;/figure&gt;

Slope histograms look like two different $$Beta$$ distributions:

```python
fig = plt.figure(figsize=(12, 8))
plt.hist(beta_control, 50, color='r', alpha=0.5, label='Control', density=True);
plt.hist(beta_case, 50, color='b', alpha=0.5, label='Case', density=True);
plt.ylabel('Density', fontsize=15);
plt.xlabel('Betas', fontsize=15);
plt.title('Slope Coefficients By Group', fontsize=15);
plt.legend(fontsize=15);
plt.tight_layout()
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/DR.Slopes.jpg' class=&quot;center-image&quot; width=&quot;80%&quot;/&gt;
    &lt;figcaption&gt;Slope distributions for synthetic dose-response curves.&lt;/figcaption&gt;
&lt;/figure&gt;

Now we'll generate some fake dose-response curves for each of the 1000 Controls, and 1000 Cases.  We'll plot a subset of these curves to visualize our cross-group curve variability.

```python
# define synthetic dose range
doses = np.linspace(-15,15,500)
dose_min = doses.min()
shifted_dose = doses + np.abs(dose_min)

fig, (ax1) = plt.subplots(figsize=(12, 8))

ec50_control = []
ec50_case = []

for c in np.arange(S):

    control_sample = sigmoid(controls[c]['beta'], controls[c]['intercept'], doses)
    case_sample = sigmoid(cases[c]['beta'], cases[c]['intercept'], doses)

    ec50_control.append(shifted_dose[control_sample &lt; 0.5].max())
    ec50_case.append(shifted_dose[case_sample &lt; 0.5].max())

    if (c % 15) == 0:

        ax1.plot(shifted_dose, control_sample, c='r', linewidth=3, alpha=0.3)
        ax1.plot(shifted_dose, case_sample, c='b', linewidth=3, alpha=0.3)

plt.legend({})
plt.title('Dose Response Curve', fontsize=20);
plt.xlabel('Dose', fontsize=20);
plt.xticks(fontsize=15)
plt.ylabel('Response', fontsize=20);
plt.yticks(fontsize=15)

custom_lines = [Line2D([0], [0], color='r', lw=4),
                Line2D([0], [0], color='b', lw=4)]
plt.legend(custom_lines, ['Control', 'Case'], fontsize=20);
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/DR.Curves.jpg' class=&quot;center-image&quot; width=&quot;80%&quot;/&gt;
    &lt;figcaption&gt;Case/Control dose-response curves.&lt;/figcaption&gt;
&lt;/figure&gt;

For our preliminary biomarker of interest, let's look at the **ec50**, which is the dose at which *50%* of the cells show some response (i.e. where our $$y$$-axis = 0.5), for each sample in our dataset.  We'll plot these doses as a function of Cases and Controls.

```python
fig = plt.figure(figsize=(12, 8))
plt.hist(ec50_control, 50, color='r', alpha=0.5, label='Control', density=True);
plt.hist(ec50_case, 50, color='b', alpha=0.5, label='Case', density=True);
plt.legend(fontsize=20);
plt.xlabel('ec50', fontsize=20);
plt.xticks(fontsize=15);
plt.ylabel('Density', fontsize=20);
plt.yticks(fontsize=15);
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/DR.ec50.jpg' class=&quot;center-image&quot; width=&quot;80%&quot;/&gt;
    &lt;figcaption&gt;Biomarker distributions: ec50 for Cases and Controls.&lt;/figcaption&gt;
&lt;/figure&gt;

If we select a different threshold -- i.e. instead of 0.5, we can iterate over the range of 0.1 - 0.9, for example, in increments of 0.1 -- we generate different biomarkers (ec10, ec20 ... ec90).  We can treat each biomarker as a different classification model, and assess how powerful that model is at assessing whether someone will develop cardiotoxicity or not.  To do so, we'll create distributions for each biomarker (not shown), and then generate ROC curves and AUC values for each curve.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/dose-response/DR.ecCurves.All.jpg' class=&quot;center-image&quot; width=&quot;80%&quot;/&gt;
    &lt;figcaption&gt;ROC curves and AUC for each ec-X biomarker level.&lt;/figcaption&gt;
&lt;/figure&gt;

This is where my limited domain knowledge comes at a cost -- I'm not sure if the biomarkers I've chosen (i.e. incremental **ec** values) are actually biologically relevant.  The point, however, is that each biomarker yields a different AUC, which theoretically shows that the Cases and Controls can be differentially distinguished, depending on which biomarker we choose to examine.  In this case, **ec10** has the most discriminative diagnostic power.

Something I did wonder about while exploring this data was how dependent the ROC curves and AUC statistics are on sample size.  Previously, I'd looked at rates of convergence of various estimators -- the AUC should also theoretically show some convergence to a &quot;true&quot; value as $n$ increases -- but I'm not sure if it follows any sort of relevant distribution.  I imagine the AUC is domain-dependent, in that it depends on the distribution of the biomarker of interest?  Might be a good idea for another post...

Cheers.</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">The other day, one of my friends and fellow PhD colleagues (I’ll refer to him as Dr. A) asked me if I knew anything about assessing biomarker diagnostic power. He went on to describe his clinical problem. I’ll try to describe his research problem here (but will likely mess up some of the relevant details, as his research pertains to generating induced pluripotent cardiac stem cells, which I have little to no experience with):</summary></entry><entry><title type="html">Quick Note: Initialize Python List With Prespecified Size</title><link href="http://localhost:4444/2019/08/list-size" rel="alternate" type="text/html" title="Quick Note: Initialize Python List With Prespecified Size" /><published>2019-08-20T18:12:32-07:00</published><updated>2019-08-20T18:12:32-07:00</updated><id>http://localhost:4444/2019/08/list-size</id><content type="html" xml:base="http://localhost:4444/2019/08/list-size">I wanted to make a quick note about something I found incredibly helpful the other day.

Lists (or ArrayLists, as new Computer Science students are often taught in their CS 101 courses), are a data strucures that are fundamentally based on arrays, but with additional methods associated with them.  Lists are generally filled with an ```append``` method, that fills indices in this array.  Lists are often useful in the case where the number of intial spots that will be filled is unknown.  

The base arrays are generally associated with a ```size``` or ```length``` parameter, that initializes the array to a certain length.  Under the hood (and generally hidden from the user), however, the ```List``` class also has a ```resize``` method that adds available space to the array when a certain percentage of available indices are occupied, technically allowing the size of the list to grow, and grow, and grow...

Perptually applying ```resize```, however, is slow, especially in the case where you're appending a lot of items.  All of the data currently in the ```List``` object will need to be moved into the new, resized array.

I needed to aggregate a large number (couple thousand) of Pandas DataFrame objects, each saved as a single file, into a single DataFrame.  My first thought was to simply incrementally load and append all incoming DataFrames to a list, and then use ```pandas.concat``` to aggregate them all together.  Appending all of these DataFrames together became incredibly time consuming (at this point, I remembered the ```resize``` issue).

A quick Google search led me to the following solution, allowing me to predefine how large I wanted my list to be:

```python
# For simplicity assume we have 10 items
known_size = 10
initialized_list = [None]*known_size

print(len(initialized_list))
10
```

Neat, huh?  And ridiculously simple.  Now, rather than append, we can do the following:

```python
for j, temp_file in enumerate(list_of_files):
    loaded_file = load_file(temp_file)
    initialized_list[j] = loaded_file
```

Because the **memory has already been pre-allocated**, the ```resize``` method is never accessed, and we save time.  I also found [this blog post](http://zwmiller.com/blogs/python_data_structure_speed.html) with some information about timing with regards to Numpy arrays, lists, and tuples -- the author shows that indexing into a Numpy array is actually slower than indexing into a list.  Numpy arrays are primarilly useful in the case where operations can be vectorized -- then they're the clear winners in terms of speed.</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">I wanted to make a quick note about something I found incredibly helpful the other day.</summary></entry><entry><title type="html">The Delta Method</title><link href="http://localhost:4444/2019/03/delta-method" rel="alternate" type="text/html" title="The Delta Method" /><published>2019-03-19T05:43:32-07:00</published><updated>2019-03-19T05:43:32-07:00</updated><id>http://localhost:4444/2019/03/delta-method</id><content type="html" xml:base="http://localhost:4444/2019/03/delta-method">Here, we'll look at various applications of the [Delta Method](https://en.wikipedia.org/wiki/Delta_method), especially in the context of variance stabilizing transformations, along with looking at the confidence intervals of estimates.

The Delta Method is used as a way to approximate the [Standard Error](https://en.wikipedia.org/wiki/Standard_error) of transformations of random variables, and is based on a [Taylor Series](https://en.wikipedia.org/wiki/Taylor_series) approximation.

In the univariate case, if we have a random variable, $$X_{n}$$, that converges in distribution to a $$N(0, \sigma^{2})$$ distribution, we can apply a function to this random variable as:

$$\begin{align}
\sqrt{n}(X_{n} - \theta) \xrightarrow{d} N(0,\sigma^{2}) \\
\sqrt{n}(g(X_{n}) - g(\theta)) \xrightarrow{d} \; ?
\end{align}$$

However, we don't know the asymptotic variance of this transformed variable just yet.  In this case, we can approximate our function $$g(x)$$ using a Taylor Series approximation, evaluated at $$\theta$$:

$$\begin{align}
g(x) = g(\theta) + g\prime(\theta)(x-\theta) + O()
\end{align}$$

where $$O()$$ is the remainder of higher-order Taylor Series terms that converges to 0.

By [Slutsky's Theorem](https://en.wikipedia.org/wiki/Slutsky%27s_theorem) and the [Continious Mapping Theorem](https://en.wikipedia.org/wiki/Continuous_mapping_theorem), we know that since $$\bar{\theta} \xrightarrow{p} \theta$$, we know that $$g\prime(\bar{\theta}) \xrightarrow{p} g\prime(\theta)$$

Plugging this back in to our original equation and applying Slutsky's Perturbation Theorem, we have:

$$\begin{align}
&amp;= \sqrt{n}(\Big[g(\theta) + g\prime(\theta)(x-\theta)\Big] - g(\theta)) \\
&amp;= \sqrt{n}(g\prime(\theta)(x-\theta)) \\
&amp;= g\prime(\theta)\sqrt{n}(X_{n} - \theta)
\end{align}$$

and since we know that $$\sqrt{n}(\bar{X_{n}} - \theta)  \xrightarrow{d} N(0,\sigma^{2})$$, we now know that $$g\prime(\theta) \sqrt{n}(\bar{X_{n}} - \theta) \xrightarrow{d} N(0,g\prime(\theta)^{2} \sigma^{2})$$.  As such, we have that:

$$\begin{align}
\sqrt{n}(g(X_{n}) - g(\theta)) \xrightarrow{d} N(0, g\prime(\theta)^{2}\sigma^{2})
\end{align}$$

The Delta Method can be generalized to the multivariate case, where, instead of the derivative, we use the gradient vector of our function:

$$\begin{align}
\sqrt{n}(g(\bar{X_{n}} - g(\theta)) \xrightarrow{d} N(0, \nabla(g)^{T} \Sigma \nabla(g))
\end{align}$$

Below, I'm going to look at a few examples applying the Delta Method to simple functions of random variables.  Then I'll go into more involved examples applying the Delta Method via [Variance Stabilizing Transformations](https://en.wikipedia.org/wiki/Variance-stabilizing_transformation).  Oftentimes, the variance of an estimate depends on its mean, which can vary with the sample size.  In this case, we'd like to find a function $$g(\theta)$$, such that, when applied via the Delta Method, the variance is constant as a function of the sample size.

We'll start by importing the necessary libraries and defining two functions:

```python
%matplotlib inline
import matplotlib.pyplot as plt

from matplotlib import rc
rc('text', usetex=True)

from scipy.stats import norm, poisson, expon
import numpy as np
```

Here, we define two simple functions -- one to compute the difference between our estimate and its population paramter, and the other to compute the function of our random variable as described by the Central Limit Theorem.

```python
def conv_prob(n, est, pop):
    
    &quot;&quot;&quot;
    Method to compute the estimate for convergence in probability.
    &quot;&quot;&quot;
    
    return (est-pop)

def clt(n, est, pop):
    
    &quot;&quot;&quot;
    Method to examine the Central Limit Theorem.
    &quot;&quot;&quot;
    
    return np.sqrt(n)*(est-pop)
```

Let's have a look at an easy example with the Normal Distribution.  We'll set $$\mu = 0$$ and $$\sigma^{2} = 5$$.  Remember that when using the ```Scipy``` Normal distribution, the ```norm``` class accepts the **standard deviation**, not the variance.  We'll show via the Central Limit Theorem that the function $$\sqrt{n}(\bar{X_{n}} - \mu) \xrightarrow{d} N(0,\sigma^{2})$$.

```python
# set sample sample sizes, and number of sampling iterations
N = [5,10,50,100,500,1000]
iters = 500

mu = 0; sigma = np.sqrt(5)

# store estimates
norm_clt = {n: [] for n in N}

samples = norm(mu,sigma).rvs(size=(iters,1000))

for n in N:
    for i in np.arange(iters):
        
        est_norm = np.mean(samples[i,0:n])
        norm_clt[n].append(clt(n, est_norm, mu))
```

Now let's plot the results.

```python
# Plot results using violin plots
fig = plt.subplots(figsize=(8,5))

for i,n in enumerate(N):
    temp = norm_clt[n]
    m = np.mean(temp)
    v = np.var(temp)
    print('Sample Size: %i has empirical variance: %.2f' % (n, v.mean()))
        
    plt.violinplot(norm_clt[n], positions=[i],)
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/Normal_CLT.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Central Limit Theorem applied to Normal Distribution.&lt;/figcaption&gt;
&lt;/figure&gt;

As expected, we see that the Normal distribution mean and variance estimates are independent of the sample size.  In this case, we don't need to apply a variance stabiliing transformation.  We also see that the variance fluctuates around $$5$$.  Now, let's apply a simple function $$g(\theta) = \theta^{2}$$ to our data.  So $$g\prime(\theta) = 2\theta$$, and the variance of our function becomes $$g\prime(\mu)^{2}\sigma^{2} = (2\mu)^{2} \sigma^{2} = 4\mu^{2}\sigma^{2}$$.  Let's look at a few plots, as a function of changing $$\mu$$.

```python
# set sample sample sizes, and number of sampling iterations
mus = [1,2,3,4]

N = [5,10,50,100,500,1000]
iters = 2000
sigma = np.sqrt(5)


fig, ([ax1,ax2], [ax3,ax4]) = plt.subplots(2,2, figsize=(14,9))
for j ,m in enumerate(mus):
    
    # store estimates
    norm_clt = {n: [] for n in N}
    samples = norm(m, sigma).rvs(size=(iters, 1000))
    
    
    plt.subplot(2,2,j+1)
    for k, n in enumerate(N):
        np.random.shuffle(samples)
        for i in np.arange(iters):

            est_norm = np.mean(samples[i, 0:n])
            norm_clt[n].append(clt(n, est_norm**2, m**2))

        plt.violinplot(norm_clt[n], positions=[k],)
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/Normal_Squared.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Central Limit Theorem applied to function of Normal Distribution.&lt;/figcaption&gt;
&lt;/figure&gt;

We see that the variance increases as the mean increases, and that, as the sample sizes increase, the distributions converge to the $$N(0, 4\mu^{2}\sigma^{2})$$ asymptotic distribution.

#### Variance Stabilization for the Poisson Distribution

Now let's look at an example where the variance depends on the sample size.  We'll use the Poisson distribution in this case.  We know that for the Poisson distribution, the variance is dependent on the mean, so let's define a random variable, $$X_{\lambda}$$, where $$\lambda = n*\theta$$.  $$n$$ is the sample size, and $$\theta$$ is a fixed constant.

We'll define $$ X_{\lambda } = \sum_{i=1}^{n} X_{\theta}$$, the sum of $$n$$ independent Poisson random variables, so that the expected value and variance of $$X_{\lambda } = n\theta$$

If we wanted to apply the Central Limit Theorem to $$X_{\lambda }$$, our convergence would be as follows:

$$\begin{align}
\sqrt{n}(X_{\lambda} - \lambda) \xrightarrow{d} N(0,\sigma^{2}(\lambda))
\end{align}$$

where the variance $$\sigma^{2}(\lambda)$$ depends on the mean, $$\lambda$$.  In order to stabilize the variance of this variable, we can apply the [Delta Method](https://en.wikipedia.org/wiki/Delta_method), in order to generate a variable that converges to a standard Normal distribution asymptotically.

$$\begin{align}
\sqrt{n}(g(X_{\lambda}) - g(\lambda)) \xrightarrow{d} N(0,g\prime(\theta)^{2}\sigma^{2}) \\
\end{align}$$

where

$$\begin{align}
&amp;g\prime(\theta)^{2} \theta = 1 \\
&amp;g\prime(\theta)^{2} = \frac{1}{\theta} \\
&amp;g\prime(\theta) = \frac{1}{\sqrt{\theta}} \\
&amp;g(\theta) = \int \frac{\partial{\theta}}{\sqrt{\theta}} \\
&amp;g(\theta) = 2\sqrt{\theta}
\end{align}$$

is our variance stabilizing function.

```python
def p_lambda(n, theta=0.5):
    
    &quot;&quot;&quot;
    Function to compute lambda parameter for Poisson distribution.
    Theta is constant.
    &quot;&quot;&quot;
    return n*theta
```

```python
theta = 0.5

N = [5,10,50,100,250,500,750,1000]
iters = 500

clt_pois = {n: [] for n in N}
pois_novar= {n: [] for n in N}
pois_var = {n: [] for n in N}

for n in N:
    for i in np.arange(iters):
        est_mu = np.mean(poisson(mu=(n*theta)).rvs(n))

        pois_novar[n].append(clt(n, est_mu, p_lambda(n)))
        pois_var[n].append(clt(n, 2*np.sqrt(est_mu), 2*np.sqrt(p_lambda(n))))
        
        clt_pois[n].append(conv_prob(n, est_mu, n*theta))
```

```python
fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6))

plt.subplot(1,2,1)
for i,n in enumerate(N):
    plt.violinplot(pois_novar[n], positions=[i])

plt.subplot(1,2,2)
for i,n in enumerate(N):
    plt.violinplot(pois_var[n], positions=[i])
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/Poisson.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Variance stabilization of Poisson distribution.&lt;/figcaption&gt;
&lt;/figure&gt;

#### Variance Stabilization for the Exponential Distribution

Applying the same method to the Exponential distribtuion, we'll find that the variance stabilizing transformation is $$g(\theta) = log(\theta)$$.  We'll apply that here:

```python
theta = 0.5

N = [5,10,50,100,250,500,750,1000]
iters = 500

clt_exp = {n: [] for n in N}
exp_novar= {n: [] for n in N}
exp_var = {n: [] for n in N}

for n in N:
    for i in np.arange(iters):
        samps = expon(scale=n*theta).rvs(n)
        
        est_mu = np.mean(samps)
        est_var = np.var(samps)

        exp_novar[n].append(clt(n, est_mu, (n*theta)))
        exp_var[n].append(clt(n, np.log(est_mu), np.log(n*theta)))
        
        clt_exp[n].append(conv_prob(n, est_mu, n*theta))
```

```python
fig,([ax1, ax2]) = plt.subplots(2,1, figsize=(15,6))

plt.subplot(1,2,1)
for i,n in enumerate(N):
    plt.violinplot(exp_novar[n], positions=[i])

plt.subplot(1,2,2)
for i,n in enumerate(N):
    plt.violinplot(exp_var[n], positions=[i])
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/Exponential.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Variance stabilization of Exponential distribution.&lt;/figcaption&gt;
&lt;/figure&gt;

#### Example of Standard Error Computation Using Delta Method for Polynomial Regression

As an example of applying the Delta Method to a real-world dataset,  I've downloaded the [**banknote**](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) dataset from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).  In this exercise, I'll apply the [logistic function](https://en.wikipedia.org/wiki/Logistic_function) via logistic regression to assess whether or not a banknote is real or fake, using a set of features.   I'll compute confidence intervals of our prediction probabilities using the Delta Method.  There are four unique predictors in this case: the **variance**, **skew**, **kurtosis**, and **entropy** of the Wavelet-transformed banknote image.  I'll treat each of these predictors independently, using polynomial basis function of degree $$3$$.

In this example, we're interested in the standard error of our probability estimate.  Our function is the Logistic Function, as follows:

$$\begin{align}
g(\beta) &amp;= \frac{1}{1+e^{-x^{T}\beta}} \\
&amp;= \frac{e^{x^{T}\beta}}{1+e^{x^{T}\beta}}
\end{align}$$

where the gradient of this multivariate function is:

$$\begin{align}
\nabla g(\beta) &amp;= \frac{\partial g}{\partial \beta} e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} \\
&amp;= x^{T}e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} - x^{T}e^{x^{T}\beta}e^{x^{T}\beta} \\
&amp;= x^{T}\Big(e^{x^{T}\beta}(1+e^{x^{T}\beta})^{-1} - e^{x^{T}\beta}e^{x^{T}\beta}\Big)(1+e^{x^{T}\beta})^{-2} \\
&amp;= x^{T} \frac{e^{x^{T}\beta}}{(1+e^{x^{T}\beta})^{2}} \\
\nabla g(\beta) &amp;= x^{T} g(\beta)(1-g(\beta))
\end{align}$$

so that the final estimate of our confidence interval becomes

$$\begin{align}
&amp; \sim N(0,x^{T} g(\beta)(1-g(\beta)) \Sigma g(\beta)(1-g(\beta))x) \\
&amp; \sim N(0, \nabla g(\beta)^{T} \Sigma \nabla g(\beta))
\end{align}$$

```python
import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import PolynomialFeatures
```

```python
import pandas as pd
import statsmodels.api as sm
from sklearn.preprocessing import PolynomialFeatures

bank = pd.read_csv('/Users/kristianeschenburg/Documents/Statistics/BankNote.txt',
                   sep=',',header=None, names=['variance', 'skew', 'kurtosis', 'entropy','class'])
bank.head()

fig = plt.subplots(2,2, figsize=(12,8))
for j, measure in enumerate(['variance', 'kurtosis', 'skew', 'entropy']):

    predictor = np.asarray(bank[measure])
    response = np.asarray(bank['class'])
    
    idx = (response == 1)

    # plot test set
    plt.subplot(2,2,j+1)
    plt.violinplot(predictor[idx], positions=[1]);
    plt.violinplot(predictor[~idx], positions=[0])
    plt.title('{:} By Classification'.format(measure), fontsize=18)
    plt.ylabel('Measure: {:}'.format(measure),fontsize=15)
    plt.yticks(fontsize=13)
    plt.xticks([0,1],['Fake','Real'], fontsize=15)

plt.tight_layout()
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/bank_notes.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Bank note feature distributions, based on note class.&lt;/figcaption&gt;
&lt;/figure&gt;

Based on the above plot, we can see that **variance**, **skew**, and **kurtosis** seem to be the most informative, while the **entropy** distributions do not seem to be that different based on bank note class.

Next, we fit a logistic regression model of note classification on note feature, with polynomial order of degree 3.  We then compute the standard errors of the transformed variance.  It was transformed using the **logistic function**, so we'll need to compute the gradient of this function.

```python
fig = plt.subplots(2,2, figsize=(12,8))
for j, measure in enumerate(['variance', 'kurtosis', 'skew', 'entropy']):

    # Generate polynomial object to degree 
    # transform age to 4-degree basis function
    poly = PolynomialFeatures(degree=2)
    idx_order = np.argsort(bank[measure])

    predictor = bank[measure][idx_order]
    response = bank['class'][idx_order]

    features = poly.fit_transform(predictor.values.reshape(-1,1));

    # fit logit curve to curve
    logit = sm.Logit(response, features).fit();
    
    test_features = np.linspace(np.min(predictor), np.max(predictor), 100)
    test_features = poly.fit_transform(test_features.reshape(-1,1))
    # predict on test set
    class_prob = logit.predict(test_features)

    cov = logit.cov_params()
    yx = (class_prob*(1-class_prob))[:,None] * test_features
    se = np.sqrt(np.diag(np.dot(np.dot(yx, cov), yx.T)))

    # probability can't exceed 1, or be less than 0
    upper = np.maximum(0, np.minimum(1, class_prob+1.96*se))
    lower = np.maximum(0, np.minimum(1, class_prob-1.96*se))

    # plot test set
    plt.subplot(2,2,j+1)
    plt.plot(test_features[:, 1], class_prob);
    plt.plot(test_features[:, 1], upper, color='red', linestyle='--', alpha=0.5);
    plt.plot(test_features[:, 1], lower, color='red', linestyle='--', alpha=0.5);
    plt.title(r'P(isReal \Big| X)', fontsize=18)
    plt.xlabel('{:}'.format(measure),fontsize=15)
    plt.ylabel('Probability',fontsize=15)
    plt.grid(True)

plt.tight_layout()
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/delta_method/bank_notes_CI.jpg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Confidence intervals for each feature, computed using Delta Method.&lt;/figcaption&gt;
&lt;/figure&gt;</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">Here, we’ll look at various applications of the Delta Method, especially in the context of variance stabilizing transformations, along with looking at the confidence intervals of estimates.</summary></entry><entry><title type="html">Mahalanobis Distance: A Distributional Exploration of Brain Connectivity</title><link href="http://localhost:4444/2018/12/mahalanobis" rel="alternate" type="text/html" title="Mahalanobis Distance: A Distributional Exploration of Brain Connectivity" /><published>2018-12-06T21:12:32-08:00</published><updated>2018-12-06T21:12:32-08:00</updated><id>http://localhost:4444/2018/12/mahalanobis</id><content type="html" xml:base="http://localhost:4444/2018/12/mahalanobis">For one of the projects I'm working on, I have an array of multivariate data relating to brain connectivity patterns.  Briefly, each brain is represented as a surface mesh, which we represent as a graph $$G = (V,E)$$, where $$V$$ is a set of $$n$$ vertices, and $$E$$ are the set of edges between vertices.

Additionally, for each vertex $$v \in V$$, we also have an associated scalar *label*, which we'll denote $$l(v)$$, that identifies what region of the cortex each vertex belongs to, the set of regions which we define as $$L = \{1, 2, ... k\}$$.  And finally, for each vertex $$v \in V$$, we also have a multivariate feature vector $$r(v) \in \mathbb{R}^{1 \times k}$$, that describes the strength of connectivity between it, and every region $$l \in L$$.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/mahalanobis/parcellation.png' class=&quot;center-image&quot; width=&quot;120%&quot;/&gt;
    &lt;figcaption&gt;Example of cortical map, and array of connectivity features.&lt;/figcaption&gt;
&lt;/figure&gt;

I'm interested in examining how &quot;close&quot; the connectivity samples of one region, $$l_{j}$$, are to another region, $$l_{k}$$.  In the univariate case, one way to compare a scalar sample to a distribution is to use the $$t$$-statistic, which measures how many standard deviations away from the mean a given sample is:

$$\begin{align}
t_{s} = \frac{\bar{x} - \mu}{\frac{s}{\sqrt{n}}}
\end{align}$$

where $$\mu$$ is the population mean, and $$s$$ is the sample standard deviation.  If we square this, we get:

$$\begin{align}
t^{2} = \frac{(\bar{x} - \mu)^{2}}{\frac{s^{2}}{n}} =  \frac{n (\bar{x} - \mu)^{2}}{S^{2}} \sim F(1,n)
\end{align}$$

We know the last part is true, because the numerator and denominator are independent $$\chi^{2}$$ distributed random variables.  However, I'm not working with univariate data -- I have multivariate data.  The multivariate generalization of the $$t$$-statistic is the [Mahalanobis Distance](https://en.wikipedia.org/wiki/Mahalanobis_distance):

$$\begin{align}
d &amp;= \sqrt{(\bar{x} - \mu)\Sigma^{-1}(\bar{x}-\mu)^{T}}
\end{align}$$

where the squared Mahalanobis Distance is:

$$\begin{align}
d^{2} &amp;= (\bar{x} - \mu)\Sigma^{-1}(\bar{x}-\mu)^{T}
\end{align}$$

where $$\Sigma^{-1}$$ is the inverse covariance matrix.  If our $$X$$'s were initially distributed with a multivariate normal distribution, $$N_{p}(\mu,\Sigma)$$ (assuming $$\Sigma$$ is non-degenerate i.e. positive definite), the squared Mahalanobis distance, $$d^{2}$$ has a $$\chi^{2}_{p}$$ distribution.  We show this below.

We know that $$(X-\mu)$$ is distributed $$N_{p}(0,\Sigma)$$.  We also know that, since $$\Sigma$$ is symmetric and real, that we can compute the eigendecomposition of $$\Sigma$$ as:

$$\begin{align}
\Sigma = U \Lambda U^{T} \\
\end{align}$$

and consequentially, because $$U$$ is an orthogonal matrix, and because $$\Lambda$$ is diagonal, we know that $$\Sigma^{-1}$$ is:

$$\begin{align}
\Sigma^{-1} &amp;= (U \Lambda U^{T})^{-1} \\
&amp;= U \Lambda^{-1} U^{T} \\
&amp;= (U \Lambda^{\frac{-1}{2}}) (U \Lambda^{\frac{-1}{2}})^{T} \\
&amp;= R R^{T}
\end{align}$$

Therefore, we know that $$R^{T}(X-\mu) \sim N_{p}(0,I_{p})$$:

$$\begin{align}

X &amp;\sim N_{p}(\mu,\Sigma) \\
(X-\mu) = Y &amp;\sim N_{p}(0,\Sigma)\\
R^{T}Y = Z &amp;\sim N_{p}(0, R^{T} \Sigma R) \\
&amp;\sim N_{p}(0, \Lambda^{\frac{-1}{2}} U^{T} (U \Lambda U^{T}) U \Lambda^{\frac{-1}{2}}) \\
&amp;\sim N_{p}(0, \Lambda^{\frac{-1}{2}} I_{p} \Lambda I_{p} \Lambda^{\frac{-1}{2}}) \\
&amp;\sim N_{p}(0,I_{p})
\end{align}$$

so that we have

$$\begin{align}
&amp;= (X-\mu)\Sigma^{-1}(X-\mu)^{T} \\
&amp;= (X-\mu)RR^{T}(X-\mu)^{T} \\
&amp;= Z^{T}Z
\end{align}$$

the sum of $$p$$ standard Normal random variables, which is the definition of a $$\chi_{p}^{2}$$ distribution with $$p$$ degrees of freedom.  So, given that we start with a $$MVN$$ random variable, the squared Mahalanobis distance is $$\chi^{2}_{p}$$ distributed.  Because the sample mean and sample covariance are consistent estimators of the population mean and population covariance parameters, we can use these estimates in our computation of the Mahalanobis distance.

Also, of particular importance is the fact that the Mahalanobis distance is **not symmetric**.  That is to say, if we define the Mahalanobis distance as:

$$\begin{align}
M(A, B) = \sqrt{(A - \mu(B))\Sigma(B)^{-1}(A-\mu(B))^{T}}
\end{align}$$

then $$M(A,B) \neq M(B,A)$$, clearly.  Because the parameter estimates are not guaranteed to be the same, it's straightforward to see why this is the case.

Now, back to the task at hand.  For a specified target region, $$l_{T}$$, with a set of vertices, $$V_{T} = \{v \; : \; l(v) \; = \; l_{T}, \; \forall \; v \in V\}$$, each with their own distinct connectivity fingerprints, I want to explore which areas of the cortex have connectivity fingerprints that are different from or similar to $$l_{T}$$'s features, in distribution.  I can do this by using the Mahalanobis Distance.  And based on the analysis I showed above, we know that the data-generating process of these distances is related to the $$\chi_{p}^{2}$$ distribution.

First, I'll estimate the covariance matrix, $$\Sigma_{T}$$, of our target region, $$l_{T}$$, using the [Ledoit-Wolf estimator](http://perso.ens-lyon.fr/patrick.flandrin/LedoitWolf_JMA2004.pdf) (the shrunken covariance estimate has been shown to be a more reliable estimate of the population covariance), and mean connectivity fingerprint, $$\mu_{T}$$.  Then, I'll compute $$d^{2} = M^{2}(A,A)$$ for every $$\\{v: v \in V_{T}\\}$$.  The empirical distribution of these distances should follow a $$\chi_{p}^{2}$$ distribution.  If we wanted to do hypothesis testing, we would use this distribution as our null distribution.  

Next, in order to assess whether this intra-regional similarity is actually informative, I'll also compute the similarity of $$l_{T}$$ to every other region, $$\\{ l_{k} \; : \; \forall \; k \in L \setminus \\{T\\} \\}$$ -- that is, I'll compute $$M^{2}(A, B) \; \forall \; B \in L \setminus T$$.  If the connectivity samples of our region of interest are as similar to one another as they are to other regions, then $$d^{2}$$ doesn't really offer us any discriminating information -- I don't expect this to be the case, but we need to verify this.

Then, as a confirmation step to ensure that our empirical data actually follows the theoretical $$\chi_{p}^{2}$$ distribution, I'll compute the location and scale [Maximumim Likelihood](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation)(MLE) parameter estimates of our $$d^{2}$$ distribution, keeping the *d.o.f.* (e.g. $$p$$) fixed.

See below for Python code and figures...

### Step 1: Compute Parameter Estimates

```python
%matplotlib inline
import matplotlib.pyplot as plt

from matplotlib import rc
rc('text', usetex=True)

import numpy as np
from scipy.spatial.distance import cdist
from scipy.stats import chi2, probplot

from sklearn import covariance
```

```python
# lab_map is a dictionary, mapping label values to sample indices
# our region of interest has a label of 8
LT = 8

# get indices for region LT, and rest of brain
lt_indices = lab_map[LT]
rb_indices = np.concatenate([lab_map[k] for k in lab_map.keys() if k != LT])

data_lt = conn[lt_indices, :]
data_rb = conn[rb_indices, :]

# fit covariance and precision matrices
# Shrinkage factor = 0.2
cov_lt = covariance.ShrunkCovariance(assume_centered=False, shrinkage=0.2)
cov_lt.fit(data_lt)
P = cov_lt.precision_
```

Next, compute the Mahalanobis Distances:

```python
# LT to LT Mahalanobis Distance
dist_lt = cdist(data_lt, data_lt.mean(0)[None,:], metric='mahalanobis', VI=P)
dist_lt2 = dist_lt**2

# fit covariance estimate for every region in cortical map
EVs = {l: covariance.ShrunkCovariance(assume_centered=False, 
        shrinkage=0.2) for l in labels}

for l in lab_map.keys():
    EVs[l].fit(conn[lab_map[l],:])

# compute d^2 from LT to every cortical region
# save distances in dictionary
lt_to_brain = {}.fromkeys(labels)
for l in lab_map.keys():

    temp_data = conn[label_map[l], :]
    temp_mu = temp_data.mean(0)[None, :]

    temp_mh = cdist(data_lt, temp_mu, metric='mahalanobis', VI=EVs[l].precision_)
    temp_mh2 = temp_mh**2

    lt_to_brain[l] = temp_mh2

# plot distributions seperate (scales differ)
fig = plt.subplots(2,1, figsize=(12,12))
plt.subplot(2,1,1)
plt.hist(lt_to_brain[LT], 50, density=True, color='blue', 
    label='Region-to-Self', alpha=0.7)

plt.subplot(2,1,2)
for l in labels:
    if l != LT:
        plt.hist(lt_to_brain[l], 50, density=True, linewidth=2, 
            alpha=0.4, histtype='step')
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/mahalanobis/IntraInterMahal.jpg' class=&quot;center-image&quot;/&gt;
    &lt;figcaption&gt;Empirical distributions of within-region (top) and between-region (bottom) $$d^{2}$$ values.  Each line is the distribution of the distance of samples in our ROI to a whole region.&lt;/figcaption&gt;
&lt;/figure&gt;

As expected, the distribution of $$d^{2}$$ the distance of samples in our region of interest, $$l_{T}$$, to distributions computed from other regions are (considerably) larger and much more variable, while the profile of points within $$l_{T}$$ looks to have much smaller variance -- this is good!  This means that we have high intra-regional similarity when compared to inter-regional similarities.  This fits what's known in neuroscience as the [&quot;cortical field hypothesis&quot;](https://www.ncbi.nlm.nih.gov/pubmed/9651489).

### Step 2: Distributional QC-Check

Because we know that our data should follow a $$\chi^{2}_{p}$$ distribution, we can fit the MLE estimate of our location and scale parameters, while keeping  the $$df$$ parameter fixed.

```python
p = data_lt.shape[1]
mle_chi2_theory = chi2.fit(dist_lt2, fdf=p)

xr = np.linspace(data_lt.min(), data_lt.max(), 1000)
pdf_chi2_theory(xr, *mle_chi2_theory)

fig = plt.subplot(1,2,2,figsize=(18, 6))

# plot theoretical vs empirical null distributon
plt.subplot(1,2,1)
plt.hist(data_lt, density=True, color='blue', alpha=0.6,
    label = 'Empirical')
plt.plot(xr, pdf_chi2_theory, color='red',
    label = '$\chi^{2}_{p}')

# plot QQ plot of empirical distribution
plt.subplot(1,2,2)
probplot(D2.squeeze(), sparams=mle_chi2_theory, dist=chi2, plot=plt);
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/mahalanobis/Density.QQPlot.png' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Density and QQ plot of null distribution.&lt;/figcaption&gt;
&lt;/figure&gt;

From looking at the QQ plot, we see that the empirical density fits the theoretical density pretty well, but there is some evidence that the empirical density has heavier tails.  The heavier tail of the upper quantile could probability be explained by acknowledging that our starting cortical map is not perfect (in fact there is no &quot;gold-standard&quot; cortical map).  Cortical regions do not have discrete cutoffs, although there are reasonably steep [gradients in connectivity](https://www.ncbi.nlm.nih.gov/pubmed/25316338).  If we were to include samples that were considerably far away from the the rest of the samples, this would result in inflated densities of higher $$d^{2}$$ values.  

Likewise, we also made the distributional assumption that our connectivity vectors were multivariate normal -- this might not be true -- in which case our assumption that $$d^{2}$$ follows a $$\chi^{2}_{p}$$ would also not hold.

Finally, let's have a look at some brains!  Below, is the region we used as our target -- the connectivity profiles from vertices in this region were used to compute our mean vector and covariance matrix -- we compared the rest of the brain to this region.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/mahalanobis/Region_LT.png' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Region of interest.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/mahalanobis/MahalanobisDistance.png' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Estimated squared Mahalanobis distances, overlaid on cortical surface.&lt;/figcaption&gt;
&lt;/figure&gt;

Here, larger $$d^{2}$$ values are in red, and smaller $$d^{2}$$ are in black.  Interestingly, we do see pretty large variance of $$d^{2}$$ spread across the cortex -- however the values are smoothly varying, but there do exists sharp boundaries.  We kind of expected this -- some regions, though geodesically far away, should have similar connectivity profiles if they're connected to the same regions of the cortex.  However, the regions with connectivity profiles most different than our target region are not only contiguous (they're not noisy), but follow known anatomical boundaries, as shown by the overlaid boundary map.

This is interesting stuff -- I'd originally intended on just learning more about the Mahalanobis Distance as a measure, and exploring its distributional properties -- but now that I see these results, I think it's definitely worth exploring further!</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">For one of the projects I’m working on, I have an array of multivariate data relating to brain connectivity patterns. Briefly, each brain is represented as a surface mesh, which we represent as a graph , where is a set of vertices, and are the set of edges between vertices.</summary></entry><entry><title type="html">Convergence In Probability Using Python</title><link href="http://localhost:4444/2018/11/convergence" rel="alternate" type="text/html" title="Convergence In Probability Using Python" /><published>2018-11-28T05:12:32-08:00</published><updated>2018-11-28T05:12:32-08:00</updated><id>http://localhost:4444/2018/11/convergence</id><content type="html" xml:base="http://localhost:4444/2018/11/convergence">I'm going over **Chapter 5** in Casella and Berger's (CB) &quot;Statistical Inference&quot;, specifically **Section 5.5: Convergence Concepts**, and wanted to document the topic of [convergence in probability](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_probability) with some plots demonstrating the concept.

From CB, we have the definition of *convergence in probability*: a sequence of random variables $$X_{1}, X_{2}, ... X_{n}$$ converges in probability to a random variable $$X$$, if for every $$\epsilon &gt; 0$$,

$$\begin{align}
\lim_{n \to \infty} P(| X_{n} - X | \geq \epsilon) = 0 \\
\end{align}$$

Intuitively, this means that, if we have some random variable $$X_{k}$$ and another random variable $X$, the absolute difference between $$X_{k}$$ and $X$ gets smaller and smaller as $$k$$ increases.  The probability that this difference exceeds some value, $$\epsilon$$, shrinks to zero as $$k$$ tends towards infinity.  Using *convergence in probability*, we can derive the [Weak Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers#Weak_law) (WLLN):

$$\begin{align}
\lim_{n \to \infty} P(|\bar{X}_{n} - \mu | \geq \epsilon) = 0
\end{align}$$

which we can take to mean that the sample mean converges in probability to the population mean as the sample size goes to infinity.  If we have finite variance (that is $$Var(X) &lt; \infty$$), we can prove this using Chebyshev's Law

$$\begin{align}
 &amp;= P(|\bar{X}_{n} - \mu | \geq \epsilon) \\
 &amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{E\Big[(\bar{X}_{n} - \mu)^{2}\Big]}{\epsilon^{2}} \\
 &amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{Var(\bar{X_{n}})}{\epsilon^{2}} \\
 &amp;= P((\bar{X}_{n} - \mu)^{2} \geq \epsilon^{2}) \leq \frac{\sigma^{2}}{n^{2}\epsilon^{2}}
\end{align}$$

where $$\frac{\sigma^{2}}{n^{2} \epsilon^{2}} \rightarrow 0$$ as $$n \rightarrow \infty$$.  Intuitively, this means, that the sample mean converges to the population mean -- and the probability that their difference is larger than some value is bounded by the variance of the estimator.  Because we showed that the variance of the estimator (right hand side) shrinks to zero, we can show that the difference between the sample mean and population mean converges to zero.

We can also show a similar WLLN result for the sample variance using Chebyshev's Inequality, as:

$$\begin{align}
S_{n}^{2} = \frac{1}{n-1} \sum_{i=1}^{n} (X_{i} - \bar{X}_{n})^{2}
\end{align}$$

using the unbiased estimator, $$S_{n}^{2}$, of $\sigma^{2}$$ as follows:

$$\begin{align}
P(|S_{n}^{2} - \sigma^{2}| \geq \epsilon) \leq \frac{E\Big[(S_{n}^{2} - \sigma^{2})^{2}\Big]}{\epsilon^{2}} = \frac{Var(S_{n}^{2})}{\epsilon^{2}}
\end{align}$$

so all we need to do is show that $$Var(S_{n}^{2}) \rightarrow 0$ as $n \rightarrow \infty$$.

Let's have a look at some (simple) real-world examples.  We'll start by sampling from a $$Normal(0,1)$$ distribution, and compute the sample mean and variance using their unbiased estimators.

```python
# Import numpy and scipy libraries
import numpy as np
from scipy.stats import norm

%matplotlib inline
import matplotlib.pyplot as plt

plt.rc('text', usetex=True)
```

```python
# Generate set of samples sizes
samples = np.concatenate([np.arange(0, 105, 5), 
                          10*np.arange(10, 110, 10),
                         100*np.arange(10, 210, 10)])

# number of repeated samplings for each sample size
iterations = 500

# store sample mean and variance
means = np.zeros((iterations, len(samples)))
vsrs = np.zeros((iterations, len(samples)))

for i in np.arange(iterations):
    for j, s in enumerate(samples):
        
        # generate samples from N(0,1) distribution
        N = norm.rvs(loc=0, scale=1, size=s)
        mu = np.mean(N)
        
        # unbiased estimate of variance
        vr = ((N - mu)**2).sum()/(s-1)

        means[i, j] = mu
        vsrs[i, j] = vr
```

Let's have a look at the sample means and variances as a function of the sample size.  Empirically, we see that both the sample mean and variance estimates converge to their population parameters, 0 and 1.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/convergence/WLLN_Mean.jpg' class=&quot;center-image&quot;/&gt;
    &lt;figcaption&gt;Sample mean estimates as a function of sample size.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/convergence/WLLN_Variance.jpg' class=&quot;center-image&quot;/&gt;
    &lt;figcaption&gt;Sample variance estimates as a function of sample size.&lt;/figcaption&gt;
&lt;/figure&gt;

Below is a simple method to compute the empirical probability that an estimate exceeds the epsilon threshold.

```python
def ecdf(data, pparam, epsilon):
    
    &quot;&quot;&quot;
    Compute empirical probability P( |estimate - pop-param| &lt; epsilon).
    
    Parameters:
    - - - - -
    data: array, float
        array of samples
    pparam: float
        true population parameter
    epsilon: float
        threshold value
    &quot;&quot;&quot;
    
    compare = (np.abs(data - pparam) &lt; epsilon)
    prob = compare.mean(0)
    
    return prob
```

```python
# test multiple epsilon thresholds
e = [0.9, 0.75, 0.5, 0.25, 0.1, 0.05, 0.01]

mean_probs = []
vrs_probs = []
# compute empirical probabilities at each threshold
for E in e:
    mean_probs.append(1 - ecdf(means, pparam=0, epsilon=E))
    vrs_probs.append(1-ecdf(vsrs, pparam=1, epsilon=E))
```

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/convergence/ECDF_Mean.jpg' class=&quot;center-image&quot;/&gt;
    &lt;figcaption&gt;Empirical probability that mean estimate exceeds population mean by epsilon. &lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/convergence/ECDF_Variance.jpg' class=&quot;center-image&quot;/&gt;
    &lt;figcaption&gt;Empirical probability that variance estimate exceeds population variance by epsilon.&lt;/figcaption&gt;
&lt;/figure&gt;

The above plots show that, as sample size increases, the mean estimator and variance estimator both converge to their true population parameters.  Likewise, examining the empirical probability plots, we can see that the probability that either estimate exceeds the epsilon thresholds shrinks to zero as the sample size increases.

If we wish to consider a stronger degree of convergence, we can consider *convergence almost surely*, which says the following:

$$\begin{align}
P(\lim_{n \to \infty} |X_{n} - X| \geq \epsilon) = 0 \\
\end{align}$$

which considers the entire joint distribution of estimates $$( X_{1}, X_{2}...X_{n}, X)$$, rather than all pairwise estimates $$(X_{1},X), (X_{2},X)... (X_{n},X)$$ -- the entire set of estimates must converge to $$X$$ as the sample size approaches infinity.</content><author><name>Kristian M. Eschenburg</name></author><category term="convergence" /><category term="probability" /><category term="almost-surely" /><summary type="html">I’m going over Chapter 5 in Casella and Berger’s (CB) “Statistical Inference”, specifically Section 5.5: Convergence Concepts, and wanted to document the topic of convergence in probability with some plots demonstrating the concept.</summary></entry><entry><title type="html">Relationship Between Poisson and Multinomial</title><link href="http://localhost:4444/2018/11/poisson-multinomial" rel="alternate" type="text/html" title="Relationship Between Poisson and Multinomial" /><published>2018-11-07T17:12:32-08:00</published><updated>2018-11-07T17:12:32-08:00</updated><id>http://localhost:4444/2018/11/poisson-multinomial</id><content type="html" xml:base="http://localhost:4444/2018/11/poisson-multinomial">In this post, I'm going to briefly cover the relationship between the Poisson distribution and the Multinomial distribution.  

Let's say that we have a set of independent, Poisson-distributed random variables $$Y_{1}, Y_{2}... Y_{k}$$ with rate parameters $$\lambda_{1}, \lambda_{2}, ...\lambda_{k}$$.  We can model the sum of these random variables as a new random variable $$N = \sum_{i=1}^{k} Y_{i}$$.

Let start with $$k=2$$.  We can define the distrbution of $$F_{N}(n)$$ as follows:

$$\begin{align}
&amp;= P(N \leq n) \\
&amp;= P(Y_{1} + Y_{2} \leq n) \\
&amp;= P(Y_{1} = y_{1}, Y_{2} = n - y_{1}) \\
&amp;= P(Y_{1} = y_{1}) \cdot P(Y_{2} = n-y_{1}) \\
&amp;= \sum_{y_{1}=0}^{n} \frac{e^{-\lambda_{1}}\lambda_{1}^{y_{1}}}{y_{1}!} \cdot \frac{e^{-\lambda_{2}}\lambda_{2}^{n-y_{1}}}{(n-y_{1})!} \\
&amp;= e^{-(\lambda_{1}+\lambda_{2})} \sum_{y_{1}=0}^{n} \frac{\lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\
&amp;= e^{-(\lambda_{1}+\lambda_{2})} \sum_{y_{1}=0}^{n} \frac{n!}{n!}\frac{\lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}}{y_{1}!(n-y_{1})!} \\
&amp;= \frac{e^{-(\lambda_{1}+\lambda_{2})}}{n!} \sum_{y_{1}=0}^{n} {n\choose y_{1}} \lambda_{1}^{y_{1}}\lambda_{2}^{n-y_{1}}
\end{align}$$

Here, we can apply the Binomial Theorem to the summation to get the following (remember that the Binomial Theorem says, for two numbers $$x$$ and $$y$$, that $$(x+y)^{n} = \sum_{i=0}^{n} {n \choose i}x^{i}y^{n-i}$$):

$$\begin{align}
\frac{e^{-(\lambda_{1}+\lambda_{2})}(\lambda_{1} + \lambda_{2})^{n}}{n!} \\
\end{align}$$

which we see is in fact just another Poisson distribution with rate parameter equal to $$\lambda_{1} + \lambda_{2}$$.  This shows that the sum of independent Poisson distributed random variables is also a Poisson random variable, with rate parameter equal to the sum of the univariate rates.  By induction, we see that for $$k$$ independent Poisson distributed random variables $$Y_{1}...Y_{k}$$, their sum $$\sum_{i=1}^{k} Y_{i} \sim Poisson(\sum_{i=1}^{k} \lambda_{i})$$.

Now let's say we're interested in modeling the conditional distribution of $$(Y_{1}...Y_{k}) \mid \sum_{i=1}^{k} = n$$.  By definition of conditional probability, we have that

$$\begin{align}
P(\bar{Y} \mid N=n) &amp;= \frac{P(\bar{Y} \; \cap \; N=n)}{P(N=n)} \\
&amp;= \frac{P(\bar{Y})}{P(N=n)}
\end{align}$$

We have the following:

$$\begin{align}
P(\bar{Y} \mid N=n) &amp;= \frac{P(\bar{Y} \; \cap \; N=n)}{P(N=n)} \\
&amp;= \Big( \prod_{i=1}^{k} \frac{e^{-\lambda_{i}} \cdot \lambda_{i}^{y_{i}}}{y_{i}!} \Big) \Big/ \frac{e^{-\sum_{i=1}^{k} \lambda_{i}}(\sum_{i}^{k} \lambda_{i})^{n}}{n!} \\
&amp;= \Big( \frac{ e^{-\sum_{i=1}^{k}} \prod_{i=1}^{k} \lambda_{i}^{y_{i}}}{\prod_{i=1}^{k} y_{i}!} \Big) \Big/ \frac{e^{-\sum_{i=1}^{k} \lambda_{i}}(\sum_{i}^{k} \lambda_{i})^{n}}{n!} \\
&amp;= { n \choose y_{1}, y_{2}, ...y_{k}} \frac{\prod_{i=1}^{k} \lambda_{i}^{y_{i}}} { \sum_{i}^{k} \lambda_{i})^{n}} \\
&amp;= { n \choose y_{1}, y_{2}, ...y_{k}}  \prod_{i=1}^{k} \Big( \frac{ \lambda_{i} }{\sum_{i}^{k} \lambda_{i}} \Big)^{y_{i}} \\
&amp;\sim MultiNom(n; \frac{\lambda_{1}}{\sum_{i=1}^{k}}, \frac{\lambda_{2}}{\sum_{i=1}^{k}}, ... \frac{\lambda_{k}}{\sum_{i=1}^{k}})
\end{align}$$

So finally, we see that, given the sum of independent Poisson random variables, that conditional distribution of each element of the Poisson vector is Multinomial distributed, with count probabilities scaled by the sum of the individual rates.  Importantly, we can extend these ideas (specifically the sum of independent Poisson random variables) to other models, such as splitting and merging homogenous and non-homogenous Poisson Point Processes.</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">In this post, I’m going to briefly cover the relationship between the Poisson distribution and the Multinomial distribution.</summary></entry><entry><title type="html">Brains, C++, and Data Science: Back to the Foundations</title><link href="http://localhost:4444/2018/10/needy_cpp" rel="alternate" type="text/html" title="Brains, C++, and Data Science: Back to the Foundations" /><published>2018-10-29T01:12:32-07:00</published><updated>2018-10-29T01:12:32-07:00</updated><id>http://localhost:4444/2018/10/needy_cpp</id><content type="html" xml:base="http://localhost:4444/2018/10/needy_cpp">While most of my day-to-day research entails writing Python code, I also make heavy use of pre-written software.  Most software comes pre-compiled, but whenever possible, I like to get access to the source code.  I'm going to refer to some modifications I made to pre-existing packages -- you can find those [in my repository here.](https://github.com/kristianeschenburg/ptx3)

The most-commonly used open-source package for brain imaging is called [FMRIB Software Library](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FSL) (FSL), which includes tools for processing MRI data, with applications ranging from motion correction and image registration, to modal decomposition methods, among many others.  All of this is made available as a set of pre-compiled C++ binaries.

I needed to modify FSL's [probtrackx2](https://fsl.fmrib.ox.ac.uk/fsl/fslwiki/FDT/UserGuide#PROBTRACKX_-_probabilistic_tracking_with_crossing_fibres) tool.  ```probtrackx2``` is a tool for generating probabilistic tractography.  Using diffusion MRI, we can model the movement of water in the brain.  At the voxel level, diffusion tends to be high when water moves along neuronal axon bundles, and low when moving against the myelin or in the extracellular matrix -- this water movement can be modeled using a variety of approaches.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/probtrackx/tractography.png' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Diffusion tractography from Biomedical Image Computing Group at USC.&lt;/figcaption&gt;
&lt;/figure&gt;

At the simplest level, the diffusion can be modeled as a [diffusion tensor](https://en.wikipedia.org/wiki/Tensor), where the [eigenvalues](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors) of the tensor correspond to the amount of diffusion in the direction of the corresponding eigenvector.  At the more complex levels, we can represent the diffusion as a 3D [probability distribution function](https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.22365), whose marginal distributions are called **orientation distribution functions** (ODF), and represent these continuous functions using a [spherical harmonics](https://en.wikipedia.org/wiki/Spherical_harmonics) basis set of the ODF.  Using ```probtrackx2```, we can sample these ODFs using a [Markov Chain Monte Carlo](https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo) approach and &quot;walk&quot; throught the brain.  Directions where the diffusion signal is high will be sampled more often, and we can generate a robust representation of the macroscale neuronal structural in the brain using these random walks.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/probtrackx/ODFs.jpeg' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Orientation distribution functions from Vega et al. 2009.&lt;/figcaption&gt;
&lt;/figure&gt;

The diffusion signal at the gray matter / white matter interface of the cortex is more isotropic than within the white matter (e.g. the diffusion tensors in these regions are more spherical).  To reduce noise in my fiber tracking results due to this low signal, **I wanted to be able to force the first steps of the streamline propagation algorithm to follow a specific direction into the white matter, before beginning the MCMC sampling procedure**. Essentially what this boils down to is providing ```probtrackx2``` with prespecified spherical coordinates (azimuthal and polar angles) for the first propagation step.  More specifically, I computed the initial spherical coordinates using surfaces computed from the mesh curvature flow results of [St.-Onge et al.](https://www.sciencedirect.com/science/article/pii/S1053811917310583)  Importantly, I wanted to make use of the ```probtrackx2``` infrastructure as much as possible e.g. I didn't want to write my own classes for loading in surface data, and wanted to minimally update the members of any other classes I found useful.

&lt;figure&gt;
    &lt;img src='{{site.baseurl}}/img/probtrackx/StOngeSurfaceFlow.png' class=&quot;center-image&quot; width=&quot;100%&quot;/&gt;
    &lt;figcaption&gt;Surface-flow seeded tractography from St-Onge et al. 2018. &lt;/figcaption&gt;
&lt;/figure&gt;

Jumping under the hood into the ```probtrackx2``` code was a **feat**.  While the software is sophistcated, it is *quite* poorly documented.  As is common with academic code, development generally begins as a way to solve a specific problem in the lab, rather than as a package to be made available for public use.  FSL has been around for a while, and grows in complexity all the time, so the initial academic-oriented mindset has somewhat propagated through their development cycles.  I was able to identify the important classes and make my modifications to these three classes:

 - ```Particle``` in particle.h :
   - performs the tracking for a single streamline for a single seed, where MCMC sampling happens


 - ```Seedmanager``` in streamlines.h :
   - manages the individual seeds, instantiates ```Particle``` objects


 - ```Counter``` in streamlines.h :
   - keeps track of streamline coordinates in 3D-space, successful streamlines, binary brain masks, saves fiber count distributions as brain volumes

The bulk of the tracking is done using these three -- the rest of the ```probtrackx2``` code is almost entirely devoted to parsing other options and handling other input data.  While I now have a lot of work to do in actually *using* my modifications, this foray into FSL's source code re-emphasized three important lessons:

 1. Documentation is **critical**.  But not just any documentation -- **meaningful** documentation.  Even if you aren't the best at object-oriented software development, at least describe what your code does, and give your variables meaningful names.  Had their code been effectively documented, I could have been in and out of there in two or three days, but instead spent about a week figuring out what was actually going on.

 2. You should be equally comfortable working with raw code developed by others, as you are writing your own.  Do not expect everything to be written correctly, and do not assume that just because others have used a piece of software before, that you won't need to make modifications.  Be ready to get your hands dirty.

 3. Do not underestimate the power of compiled languages.  Most data scientists work with Python and R due to the speed of development and low barrier to entry, but each is based primarily in C (and I believe not in C++ due to timing of original development cycles).  Many large-scale software packages are based on languages like C, C++, and Java.  Likewise, if your work bridges the gap between data scientist and engineer, you'll definitely need to be comfortable working with compiled languages for production-level development and deployment.</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">While most of my day-to-day research entails writing Python code, I also make heavy use of pre-written software. Most software comes pre-compiled, but whenever possible, I like to get access to the source code. I’m going to refer to some modifications I made to pre-existing packages – you can find those in my repository here.</summary></entry><entry><title type="html">Enabling Custom Jekyll Plugins with TravisCI</title><link href="http://localhost:4444/2018/08/custom-plugins-with-travisci" rel="alternate" type="text/html" title="Enabling Custom Jekyll Plugins with TravisCI" /><published>2018-08-11T19:14:14-07:00</published><updated>2018-08-11T19:14:14-07:00</updated><id>http://localhost:4444/2018/08/custom-plugins-with-travisci</id><content type="html" xml:base="http://localhost:4444/2018/08/custom-plugins-with-travisci">I just learned about [TravisCI](https://travis-ci.org/) (actually, about continuous integration (CI) in general) after attending [Neurohackademy 2018](http://neurohackademy.org/).  We learned about CI from the perspective of ensuring that your code builds properly when you update files in your packages, incorporate new methods, refactor your code, etc.  Pretty neat.

Fast forward a couple days, and I'm trying to incorporate custom Jekyll plugins into my blog -- I quickly realized GitHub doesn't allow this for security reasons, but I couldn't find  a convenient work-around.  Some posts suggested using a separate repo branch to build the site, and then push the static HTML files up to a remote repo to do the actual hosting, but for some reason I couldn't get that approach to work.

Finally, I saw some mentions of using TravisCI and [CircleCI](https://circleci.com/pricing/?utm_source=gb&amp;utm_medium=SEM&amp;utm_campaign=SEM-gb-200-Eng-ni&amp;utm_content=SEM-gb-200-Eng-ni-Circle-CI&amp;gclid=Cj0KCQjwtb_bBRCFARIsAO5fVvGQIO23w0ahWrTj3v8MrGLEnjI00KcEClqUuQda-Q_cz05h8jjEC5QaAjeREALw_wcB) to build and push the site using continuous integration.  I ended up using the approach suggested by [Josh Frankel](http://joshfrankel.me/blog/deploying-a-jekyll-blog-to-github-pages-with-custom-plugins-and-travisci/).

Josh's site gives a really clear explanation of the necessary steps, given some very minmal prequisite knowledge about using Git.  His instructions actually worked almost perfectly for me, so I won't repeat them again here (just follow the link above, if you're interested) -- however, there were a few issues that arose on my end:

  1. For some reason, I had an ```about.html``` file and ```index.html``` file in the main repo directory -- my built blog wouldn't register any updates I made to ```about.md``` or ```index.md``` while these files were around, so I deleted the HTML files.  This might have been an obvious bug to someone with more web programming experience, but I'm a novice at that.  If you're seeing any wonky behavior, check to make sure you don't have any unnecessary files hanging around.

  2. **Ruby version**:  I had to change the version of Ruby I was using to ```ruby-2.4.1```.

  3. **Plugins**: Make sure any Jekyll plugins you want to use are already installed.

  4. **Emails**: You can turn off email reporting from TravisCI by adding
    ```notifications: email: false``` to your ```.travis.yml``` file.

But now, you can incorporate custom, user-built Jekyll plugins and let TravisCI do the heavy lifting!  I specifically wanted the ability to reference papers using BibTex-style citation links with Jekyll, like you can with LaTex or Endnote -- this capability isn't currently supported by GitHub.  Happy blogging!</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">I just learned about TravisCI (actually, about continuous integration (CI) in general) after attending Neurohackademy 2018. We learned about CI from the perspective of ensuring that your code builds properly when you update files in your packages, incorporate new methods, refactor your code, etc. Pretty neat.</summary></entry><entry><title type="html">Rendering LaTex In Markdown Using Jekyll</title><link href="http://localhost:4444/2018/08/rendering-latex" rel="alternate" type="text/html" title="Rendering LaTex In Markdown Using Jekyll" /><published>2018-08-10T19:14:14-07:00</published><updated>2018-08-10T19:14:14-07:00</updated><id>http://localhost:4444/2018/08/rendering-latex</id><content type="html" xml:base="http://localhost:4444/2018/08/rendering-latex">In putting together this blog, I wanted to be able to talk about various mathematical topics that I found interesting, which inevitably lead to using LaTex in my posts.

I'm currently using Atom as my editor (having converted from Sublime), and needed to install a bunch of packages first.  First and foremost, I wanted to be able to render my markdown posts before hosting them on the blog, and consequentially needed a way to render LaTex.  For this, I installed a few Atom packages:

  * [Markdown-Preview](https://atom.io/packages/markdown-it-preview )
  * [Latex](https://atom.io/packages/latex)
  * [Language-Latex](https://atom.io/packages/language-latex)

To preview your post in Atom, you just type ```ctrl+shift+M```, which will display both in-line and block math sections.

However, if you build your site locally with the command ```bundle exec jekyll serve``` or push it to a remote repo, the LaTex no longer renders properly.  After Googling around a bit, I determined that this was due to the way markdown converters in Jekyll, like **kramdown** and **redcarpet**, do the conversion using MathJax -- specifically, in-line math segments are not properly rendered.  I wanted a way to both preview the LaTex in Atom, and properly render it usng Jekyll.  I found two links that solved the problem for me:

  * [Visually Enforced](http://www.gastonsanchez.com/visually-enforced/opinion/2014/02/16/Mathjax-with-jekyll/)
  * [LaTeX in Jekyll](http://www.iangoodfellow.com/blog/jekyll/markdown/tex/2016/11/07/latex-in-markdown.html)

In short, the following steps solved the problem of LaTex not rendering for me.  I'm using the **minima** theme, so I first found the theme directory with ```bundle show minima```.  In this directory, I copied the **./layouts/post.html** to a local directory in my project folder called **./\_layouts/post.html**.

Within this file, I pasted the following two sections of HTML code:

```html:
&lt;script type=&quot;text/x-mathjax-config&quot;&gt;
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        inlineMath: [['$','$']]
      }
    });
  &lt;/script&gt;
&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;
```

And voila -- building the posts now correctly renders LaTex!</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">In putting together this blog, I wanted to be able to talk about various mathematical topics that I found interesting, which inevitably lead to using LaTex in my posts.</summary></entry><entry><title type="html">Exploring Dynamical Systems With DMD: Part 2</title><link href="http://localhost:4444/2018/05/dynamic-mode-decomposition-part-2" rel="alternate" type="text/html" title="Exploring Dynamical Systems With DMD: Part 2" /><published>2018-05-24T04:30:43-07:00</published><updated>2018-05-24T04:30:43-07:00</updated><id>http://localhost:4444/2018/05/dynamic-mode-decomposition-part-2</id><content type="html" xml:base="http://localhost:4444/2018/05/dynamic-mode-decomposition-part-2">In my previous post on [Dynamic Mode Decomposition]({% post_url 2018-05-24-dynamic-mode-decomposition-part-1 %}), I discussed the foundations of DMD as a means for linearizing a dynamical system.  In this post, I want to look at a way in which we can use rank-updates to incorporate new information into the spectral decomposition of our linear operator, $$A$$, in the event that we are generating online measurements from our dynamical system.  If you want a more-detailed overview of this topic, {% cite zhang_dmd --file dmd %} developed the theory, along with open source code, for testing this method.

Recall that we are given an initial data matrix

$$\begin{align}
X = \begin{bmatrix}
x_{n_{1},m_{1}} &amp; x_{n_{1},m_{2}} &amp; x_{n_{1},m_{3}} &amp; ... \\
x_{n_{2},m_{1}} &amp; x_{n_{1},m_{2}} &amp; x_{n_{2},m_{3}} &amp; ... \\
x_{n_{3},m_{1}} &amp; x_{n_{1},m_{2}} &amp; x_{n_{3},m_{3}} &amp; ... \\
... &amp; ... &amp; ... &amp; ... \\
\end{bmatrix}
\in R^{n \times m}
\end{align}$$

which we can split into two matrices, shifted one unit in time apart:

$$\begin{align}
X^{\ast} &amp;=
\begin{bmatrix}
\vert &amp; \vert &amp; &amp; \vert \\
\vec{x}_1 &amp; \vec{x}_2  &amp; \dots &amp; \vec{x}_{m-1}  \\
\vert &amp; \vert &amp; &amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)} \\
Y &amp;= \begin{bmatrix}
\vert &amp; \vert &amp; &amp; \vert \\
\vec{x}_2 &amp; \vec{x}_3  &amp; \dots &amp; \vec{x}_{m}  \\
\vert &amp; \vert &amp; &amp; \vert \\
\end{bmatrix} \in R^{n \times (m-1)}
\end{align}$$

and we are interested in solving for the linear operator $A$, such that

$$\begin{align}
Y = AX^{\ast}
\end{align}$$

For simplicity, since we are no longer using the full matrix, I'll just refer to $$X^{\ast}$$ as $$X$$.  In the previous post, we made the constraint that $$n &gt; m$$, and that rank($$X$$) $$\leq m &lt; n$$.  Here, however, we'll reverse this assumption, and such that $$m &gt; n$$, and that rank($$X$$) $$\leq m &lt; n$$, such that $$XX^{T}$$ is invertible, so by multiplying both sides by $$X^{T}$$ we have

$$\begin{align}
AXX^{T} &amp;= YX^{T} \\
A &amp;= YX^{T}(XX^{T})^{-1} \\
A &amp;= QP_{x}
\end{align}$$

where $$Q = YX^{T}$$ and $$P_{x} = (XX^{T})^{-1}$$.  Now, let's say you observe some new data $$x_{m+1}, y_{m+1}$$, and you want to incorporate this new data into your $A$ matrix.  As in the previous post on [Rank-One Updates]({% post_url 2018-05-11-rank-one-updates %}), we saw that directly computing the inverse could potentially be costly, so we want to refrain from doing that if possible.  Instead, we'll use the Shermann-Morrison-Woodbury theorem again to incorporate our new $x_{m+1}$ sample into our inverse matrix, just as before:

$$\begin{align}
(X_{m+1}X^{T}_{m+1})^{-1} = P_{x} + \frac{P_{x}x_{m+1}x_{m+1}^{T}P_{x}}{1 + x_{m+1}^{T}P_{x}x_{m+1}}
\end{align}$$

Likewise, since we're appending new data to our $$Y$$ and $$X$$ matrices, we also have

$$\begin{align}
Y_{m+1} = \begin{bmatrix}
Y &amp; y_{m+1} \end{bmatrix} \\ \\
X_{m+1} = \begin{bmatrix}
X &amp; x_{m+1} \end{bmatrix} \\
\end{align}$$

such that

$$\begin{align}
Y_{m+1} X_{m+1}^{T} &amp;= YX^{T} + y_{m+1}x_{m+1}^{T} \\
&amp;= Q + y_{m+1}x_{m+1}^{T}
\end{align}$$

which is simply the sum of our original matrix $$Q$$, plus a rank-one matrix.  {% cite zhang_dmd --file dmd %} go on to describe some pretty cool &quot;local&quot; DMD schemes, by incorporating weights, as well as binary thresholds, that are time-dependent into the computation of the linear operator, $A$.


{% bibliography --file dmd --cited %}</content><author><name>Kristian M. Eschenburg</name></author><summary type="html">In my previous post on Dynamic Mode Decomposition, I discussed the foundations of DMD as a means for linearizing a dynamical system. In this post, I want to look at a way in which we can use rank-updates to incorporate new information into the spectral decomposition of our linear operator, , in the event that we are generating online measurements from our dynamical system. If you want a more-detailed overview of this topic, (Zhang, Rowley, Deem, &amp;amp; Cattafesta, 2017) developed the theory, along with open source code, for testing this method.</summary></entry></feed>